<!DOCTYPE html>
<html>
<head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3M0L123EEZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-3M0L123EEZ');
  </script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Refed</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide Data Synthesis </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://shuhaibm.github.io/" target="_blank">Shuhaib Mehri</a></sup>,</span>
                <span class="author-block">
                  <a href="https://xiusic.github.io/" target="_blank">Xiusi Chen</a></sup>,</span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/hengji" target="_blank">Heng Ji</a></sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/dilek" target="_blank">Dilek Hakkani-Tür</a>
                  </span>
                  </div>

                  <img alt="Conversational AI Lab" src="static/images/ConvAIText.png" style="width:10%" />

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><a href="https://uiuc-conversational-ai-lab.github.io/" target="_blank">ConvAI Lab</a><br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div> 
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Illinois at Urbana Champaign<br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2502.04511" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Shuhaibm/refed" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              <!-- Notebook LM link -->
              <span class="link-block">
                <a href="https://notebooklm.google.com/notebook/0a1118ac-e620-49a9-a44e-1168785d6a65/audio" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-headphones"></i>
                </span>
                <span>Notebook LM</span>
              </a>
            </span>


            
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            LLMs demonstrate remarkable capabilities in following natural language instructions, largely due to instruction-tuning on high-quality datasets. While synthetic data generation has emerged as a scalable approach for creating such datasets, maintaining consistent quality standards remains challenging. Recent approaches incorporate feedback to improve data quality, but typically operate at the sample level, generating and applying feedback for each response individually.
In this work, we propose <i>Reference-Level Feedback</i>, a novel methodology that instead collects feedback on high-quality reference samples from carefully curated seed data. We use this feedback to capture rich signals of desirable characteristics that can be propagated to newly synthesized data. We present REFED, a dataset of 10K instruction-response pairs synthesized using such feedback. We demonstrate the effectiveness of our approach by showing that Llama-3.1-8B-Instruct finetuned on REFED achieves state-of-the-art performance among similar-sized SFT-based models on AlpacaEval 2.0 and strong results on Arena-Hard. Through extensive experiments, we show that our approach consistently outperforms traditional sample-level feedback methods with significantly fewer feedback collections and improves performance across different model architectures.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"></h2>
        <div class="content has-text-justified">

          <p>
            Synthetic data has been a powerful driver in advancing LLMs. It enables the creation of high-quality datasets at scale, overcoming the obstacles that come with manual data collection - such as high costs, time, and effort. Building upon this, we introduce a novel methodology for synthesizing high-quality instruction-tuning datasets using <i>Reference-Level Feedback</i>.
          </p>

          <p>
            Our method revolves around collecting feedback from high-quality reference samples. This feedback captures the desirable characteristics that make these reference samples effective, and we use it to guide the synthesis process. Our experimental results demonstrate significant improvements in the synthesized data quality and efficiency compared to traditional feedback approaches.
          </p>

          <figure>
            <img src="static/images/pipeline.png" alt="Feedback comparison." style="width: 100%;"/>
            <figcaption class="has-text-centered">
              An overview of our data synthesis pipeline. Starting from our seed data, we select a reference sample and collect <i>Reference-Level Feedback</i> for both the instruction and response. Instruction feedback is used to synthesize new instructions. We generate their corresponding responses, and then improve it using the response feedback.
            </figcaption>
          </figure>
        </div>     
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reference-Level Feedback</h2>
        <div class="content has-text-justified">
          
          <p>
            Feedback is a well-known approach for improving synthetic data quality. Traditional approaches operate at the sample-level: an LLM generates a response, receives feedback (either through self-reflection or an external source), and then refines its original response. This approach has proven effective in enancing LLM performance on alignment benchmarks and reinforcing key principles such as helpfulness and truthfulness.
          </p>

          <p>
            Our method takes on a different approach by collecting feedback at the reference-level, from the carefully selected reference samples in seed data. Many approaches already use seed data as in-context examples during the synthesis process. We further leverage it by systematically analyzing the samples and capturing information about their desirable characteristics (i.e. clarity, relevance) through feedback. This feedback is then used throughout the synthesis process.
          </p>

          <figure>
            <img src="static/images/feedback.png" alt="Feedback comparison." style="width: 60%;"/>
            <figcaption class="has-text-centered">
              Comparison of feedback approaches for data synthesis. Left: Traditional sample-level feedback generates and applies feedback individually for each sample. Right: Our <i>Reference-Level Feedback</i> approach collects feedback once from a high-quality reference sample and applies it to synthesize and improve multiple new samples.
            </figcaption>
          </figure>

          <p>
            To be more specific, feedback is collected on both the instruction and response components of each reference sample. The instruction-specific feedback is used to guide the synthesis of new instructions, and response-specific feedback is used to refine the corresponding responses. Since synthesized instructions share key characteristics of their reference counterparts, response-specific feedback remains relevant and is used to improve the quality of synthesized responses. This framework enables us to systematically propagate the desirable qualities of reference samples to newly generated samples, establishing overall higher quality standards for data synthesis.
          </p>

        </div>     
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">

          <p>
            We present REFED, a high-quality instruction tuning dataset made up of 10K samples. It was created using our framework, with GPT-4o-mini as our teacher model and the LIMA training dataset as our seed data.
          </p>

          <p>
            To evaluate the effectiveness of our dataset, we finetune various language models then assess their instruction-following abilities with AlpacaEval 2.0 and Arena-Hard. These benchmarks use an LLM as a judge to compare model responses against reference responses and present metrics like the win rate or length-controlled win rate.
          </p>
          
          <h3 class="title is-5">How Effective is Reference-Level Feedback for Data Synthesis?</h3>

          <p>
            To demonstrate the effectiveness of <i>Reference-Level Feedback</i>, we finetune Llama-3.1-8B-Instruct on datasets synthesized using several approaches:
            <ul>
              <li>
                No feedback: just the seed dataset, no synthesis.
              </li>
              <li>
                Reference-Level Instruction Feedback: Using instruction feedback to synthesize new instructions, then generating a response for these instructions.
              </li>
              <li>
                Reference-Level Instruction Feedback and Sample-Level Response Feedback: Using instruction feedback to synthesize new instructions, generating a response for these instructions, collecting sample-level feedback on this response, then improving it.
              </li>
              <li>
                Reference-Level Instruction + Response Feedback: Our proposed framework, using both reference-level instruction and response feedback.
              </li>
            </ul>
          </p>

          <figure>
            <img src="static/images/table1.png" alt="Feedback comparison." style="width: 100%;"/>
            <figcaption class="has-text-centered">
              Analysis of the different components of <i>Reference-Level Feedback</i> for data synthesis. We evaluate the impact of the instruction and response feedback, and also compare against traditional sample-level feedback for response improvement. Results show performance after finetuning Llama-3.1-8B-Instruct on each generated dataset. Green subscripts indicate improvements after fine-tuning. Metrics shown are: Length-Controlled Win Rate (LC), Win Rate (WR), Standard Error (SE), Average Length (Len.), and Average # Tokens (Tok.).
            </figcaption>
          </figure>

          <p>
            The results show an increase in performance everytime we introduce a component of our framework. We also see that the models trained on datasets synthesized with <i>Reference-Level Feedback</i> outperform those trained on datasets that were synthesized with sample-level feedback.
          </p>


          <h3 class="title is-5">How Does Our Method Compare Against Other Baselines?</h3>
          
          <p>
            We compare our model against various other baselines. This includes Llama-3.1-8B-Instruct finetuned on various well-known synthetic datasets, as well as leading SFT-based, 8B-parameter models from the AlpacaEval 2.0 leaderboard. Our results shows that training on our dataset achieves state-of-the-art performance, and even sometimes outperforms significantly larger and more powerful models such as GPT-3.5 and Llama-3.1-405B-Instruct.
          </p>

          <figure>
            <img src="static/images/table2.png" alt="Feedback comparison." style="width: 85%;"/>
            <figcaption class="has-text-centered">
              Evaluation results of Llama-3.1-8B-Instruct finetuned on REFED against selected baselines. <i>Top</i> shows results from finetuning on various synthetic datasets. <i>Middle</i> shows the performance of leading models from AlpacaEval 2.0 leaderboard. <i>Bottom</i> shows our model trained on REFED. Results demonstrate that our model outperforms these baselines across both evaluation benchmarks.
            </figcaption>
          </figure>

          <h3 class="title is-5">Does REFED Generalize To Different Model Architectures?</h3>

          <p>
            We also show that finetuning on REFED yields improvements across different models (Llama-3.1-8B and Mistral-7B) for both the base and instruct variants.  Our results demonstrate consistant improvement across all model variants, with the instruct variant showing the most significant improvements. Notably, the base models finetuned on REFED either outperform or are competitive with their instruct counterparts.
          </p>

          <figure>
            <img src="static/images/table3.png" alt="Feedback comparison." style="width: 65%;"/>
            <figcaption class="has-text-centered">
              Evaluation results of finetuning REFED on the base and instruct variants of Llama-3.1-8B and Mistral-7B models. Green subscripts indicate improvements after finetuning. Note that we do not report base model performance because they are not instruction-tuned.
            </figcaption>
          </figure>

          <h3 class="title is-5">Does Filtering Enhance the Effectiveness?</h3>

          <p>
            Lastly, we explore the effectiveness of different filtering strategies:
            <ul>
              <li>
                LLM-Judge Filtering: We use an LLM judge to compare the original respone with the improved response. We keep the samples where the improved response is ranked higher than the original response.
              </li>
              <li>
                ROUGE-L Similarity Filtering: Starting wih a randomly selected sample, we iteratively add candidates where the instruction's maximum similarity score with existing instructions is below a certain threshold.
              </li>
            </ul>
            Using these methods, along with random sampling, we create samples of size 1K, 2K, 4K, and 8K. We fintune Llama-3.1-8B-Instruct on these subsets and report the length-controlled win rate in the graph below. Results indicate that LLM-Judge filtering is the most effective, with the ROUGE-L similarity filtering also showing some improvements.
          </p>
          
          <figure>
            <img src="static/images/table4.png" alt="Feedback comparison." style="width: 50%;"/>
            <figcaption class="has-text-centered">
              Length Controlled Win-Rate on AlpacaEval 2.0 for Llama-3.1-8B-Instruct finetuned on various subsets of REFED, based on different filtering strategies.
            </figcaption>
          </figure>
          
          
        </div>     
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{mehri2025samplelevelfeedbackusingreferencelevel,
        title={Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide Data Synthesis}, 
        author={Shuhaib Mehri and Xiusi Chen and Heng Ji and Dilek Hakkani-Tür},
        year={2025},
        eprint={2502.04511},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2502.04511}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Acknowledgements -->
<!-- <section class="section" id="Acknowledgements">
<div class="container is-max-desktop content">
  <h2 class="title">Acknowledgements</h2>
  <p>
    This work has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program, through which leading foundation models hosted by Microsoft Azure and access to Azure credits were provided to conduct the research.

  </p>
</section> -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website's template is borrowed from <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. We thank the authors for open-sourcing their code.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
